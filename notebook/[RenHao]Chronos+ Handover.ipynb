{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbc9c68",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook duplicates the data preprocessing and modelling that Chronos+ does, with all the database and job queue code stripped out. As far as possible, the duplication is verbatim to facilitate the transfer of any changes made in this notebook back into Chronos+. All current data rejection points are also documented.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5f3fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.tsa.stattools as sts\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "# %autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a227fa3",
   "metadata": {},
   "source": [
    "## Define CSV filename, datetime and value columns\n",
    "\n",
    "To test the modelling and scoring process on your own data, just change the following variables.\n",
    "\n",
    "1. `datecol` --> the name of the column in the CSV file we wish to use as the time series index.\n",
    "2. `ycols` --> the list of names of the value columns we wish to train and score on.\n",
    "3. `fname` --> name of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd79c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "datecol = \"Time\"\n",
    "ycols = [\"SA1900282\"] # there can be more than one\n",
    "fname = \"data/giordano-oct-nov.csv\"  # [ANDY] what's the prerequisite format of ts df (sample multivariate ts data)? can accommodate for ts1 = [0,30,60,90] and ts = [1,16,31,46]?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a8451",
   "metadata": {},
   "source": [
    "## Ingesting the data\n",
    "\n",
    "1. `datecol` must exist in the CSV or we raise an exception\n",
    "2. `ycols` must contain the name of at least one column or we raise an exception\n",
    "3. every column in `ycols` must exist in the CSV or we raise an exception\n",
    "4. if we cannot parse a value in the datetime column as a datetime, we discard the whole row\n",
    "5. every value in the columns denoted by `ycols` is parsed as a float or set to np.nan\n",
    "6. we set the `datecol` column as the index of the dataframe\n",
    "7. we sort the dataframe by the index\n",
    "7. we drop any columns not listed in `ycols` and return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481467c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingColumnError(Exception):\n",
    "    def __init__(self, col, *args, **kwargs):\n",
    "        self.column = col\n",
    "        Exception.__init__(self, *args, **kwargs)\n",
    "        \n",
    "def ingest(csv, dscol, ycols):\n",
    "    \"\"\" ingests a timeseries csv. Does not throw an error if datetime format is not followed (will always try to interpret) or if value column contains non-int or non-floats.\n",
    "\n",
    "    1) renames the datetime column to 'ds'\n",
    "\n",
    "    2) indexes the 'ds' column\n",
    "\n",
    "    3) sorts by ds column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv : str \n",
    "         path to csv with header and format `datetime,value`.\n",
    "         datetime: {%Y-%m-%d (e.g. 2017-12-01), \n",
    "                    %Y-%m-%d (e.g. 2017-12-1), \n",
    "                    %Y-%b-%d (e.g. 2017-Dec-01), \n",
    "                    %d-%b-%Y (e.g. 1-Dec-2017), \n",
    "                    %Y-%m-%d %H:%M:%S (e.g. 2017-12-01 00:00:00)}\n",
    "                 * note all represent 1st December 2017\n",
    "         value: {float, int}\n",
    "\n",
    "    dscol : str\n",
    "                    the header name for the datetime column in the csv\n",
    "    ycols: list of str\n",
    "                    the names of the value columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    if dscol not in df:\n",
    "        raise MissingColumnError(dscol)\n",
    "\n",
    "    if not ycols:\n",
    "        raise ValueError(f\"ycols is empty: {ycols}\")\n",
    "\n",
    "    for ycol in ycols:\n",
    "        if ycol not in df:\n",
    "            raise MissingColumnError(ycol)\n",
    "\n",
    "    # if the date column is not parseable as a date, then we should discard the row\n",
    "    df[dscol] = pd.to_datetime(df[dscol], errors=\"coerce\")\n",
    "    df = df[~df[dscol].isnull()]\n",
    "\n",
    "    # parse value column as float. no exceptions\n",
    "    for ycol in ycols:\n",
    "        df[ycol] = df[ycol].apply(__float_or_nan)  # [ANDY] can use pd.to_numeric(df[ycol], errors='coerce')\n",
    "\n",
    "    df = df.set_index(dscol).sort_index()[ycols]\n",
    "    return df.sort_index()  # [ANDY] redundant sort_index()?\n",
    "\n",
    "def __float_or_nan(o):\n",
    "    try:\n",
    "        return np.float(o)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0549478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest(fname, datecol, ycols)  # [ANDY] drop rows with unparseable ts but keep rows with NaN in ycols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b40687",
   "metadata": {},
   "source": [
    "## Autoencoder preprocessing\n",
    "\n",
    "This is code that needs to be run regardless of whether we are preprocessing data for training or scoring. That is why it has been refactored into its own function.\n",
    "\n",
    "1. remove duplicate rows (identical datetime and values)\n",
    "2. if we find rows with identical datetime but different values, we raise an exception because it is impossible for us to decide which row contains the correct value.\n",
    "  1. the other option is to delete all such rows, but for now we have elected to reject the entire CSV\n",
    "3. we try to detect the sampling frequency of each value column\n",
    "  1. we take the mode of the time differences between successive values for each column\n",
    "  2. if there is no mode for any of the columns we reject the whole CSV\n",
    "4. we return the resulting dataframe, along with the sampling frequencies for each column, as well as the minimum sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3afb8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_preproc(df, ycols):\n",
    "    \"\"\"takes df, ycols and returns preprocessed df, freq\"\"\"\n",
    "    df = remove_duplicate_rows(df)\n",
    "\n",
    "    dupe_ds = list(df[df.index.duplicated()].index.drop_duplicates())  # [Andy] can use sum(df.index.duplicated())\n",
    "    # we also reject if we have duplicate datetimes that are not duplicate rows\n",
    "    if len(dupe_ds)!=0:\n",
    "        raise Exception(\"rows with duplicate datetimes detected\")\n",
    "\n",
    "    colfreqs = {}\n",
    "    for y in ycols:\n",
    "        currdf = df[[y]].dropna()\n",
    "        freqs = detect_ds_frequency(currdf)\n",
    "        if len(freqs)!=1:\n",
    "            # we reject if we have no modal sampling frequency\n",
    "            raise Exception(\"more than one sampling frequency was detected\")\n",
    "\n",
    "        colfreqs[y] = freqs.index[0]\n",
    "\n",
    "    minfreq = min(colfreqs.values())\n",
    "\n",
    "    return df, colfreqs, minfreq\n",
    "\n",
    "def remove_duplicate_rows(df):\n",
    "    \"\"\"returns df with duplicates removed \n",
    "    \"\"\"    \n",
    "    idx_name = df.index.name  # [ANDY] this is the same as dscol\n",
    "    return df.reset_index().drop_duplicates().set_index(idx_name)\n",
    "\n",
    "def detect_ds_frequency(df):\n",
    "    \"\"\"detects the frequency of datetime in df.\n",
    "    \n",
    "    1) subtract all the datetime in ds by its next datetime to retrieve the deltas\n",
    "    \n",
    "    2) count the number of occurrences for each delta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame \n",
    "         The dataframe to detect the datetime frequency, needs to have a sorted datetime index\n",
    "    Returns\n",
    "    -------\n",
    "    freq : pd.Series of size 1 whose index is the modal timedelta and value is the number of times it appeared if there is one mode\n",
    "    and it appeared > 50% of the time\n",
    "           else pd.Series of max size 10 sorted by ascending order of modal timedeltas\n",
    "    \"\"\"\n",
    "    deltas = pd.Series(df.index[1:] - df.index[:-1])  # [ANDY] deltas = df.reset_index()[dscol].diff()\n",
    "    threshold = len(deltas)/2\n",
    "    delta_counts = deltas.value_counts()\n",
    "    modal_delta = delta_counts.idxmax()  # [ANDY] modal_delta = delta_counts.index[0]\n",
    "    mdcount = delta_counts[modal_delta]  # mdcount = delta_counts.iloc[0]\n",
    "\n",
    "    if mdcount > threshold:\n",
    "        return delta_counts.head(1)\n",
    "\n",
    "    dc = [item for item in delta_counts.iteritems()]\n",
    "    # sort by ascending order of timedelta\n",
    "    dc.sort(key=lambda e:e[0])\n",
    "    # sort by descending order of occurrences\n",
    "    dc.sort(key=lambda e:e[1], reverse=True)\n",
    "    dc = dc[:50]\n",
    "\n",
    "    return pd.Series(data=[e[1] for e in dc], index=[e[0] for e in dc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3329c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, colfreqs, minfreq = ae_preproc(df, ycols)  # [ANDY] df with no duplicated rows, no duplicated ts, with only one modal frequency for each column and with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d3fcb",
   "metadata": {},
   "source": [
    "## Check for sparse columns\n",
    "\n",
    "Given a duration _d_ and a time series with a sampling frequency _f_, the expected number of values it should have over _d_ is \n",
    "\n",
    "_e_ = _d_/_f_\n",
    "\n",
    "We define a sparse column as a column within the dataframe where given its sampling frequency and the duration of the whole dataframe, it has less than 75% the number of expected values. We raise an exception if any column is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6ed288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseColumnError(Exception):\n",
    "    pass\n",
    "\n",
    "def check_sparse_cols(df, colfreqs):\n",
    "    totalduration = df.index[-1] - df.index[0]\n",
    "    for y, freq in colfreqs.items():\n",
    "        numpoints = len(df[[y]].dropna())\n",
    "        # every column must cover 75% of the duration\n",
    "        expected = totalduration // freq * 0.75\n",
    "        if numpoints < expected:\n",
    "            raise SparseColumnError(f\"column {y} has frequency {freq} and {numpoints} points, but duration of dataset is {totalduration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f9ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sparse_cols(df, colfreqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f7020",
   "metadata": {},
   "source": [
    "## Get period\n",
    "\n",
    "We try to calculate the period of the data, but in order to keep things simple, we also require that the sampling frequencies of all the time series in the data (i.e. everything in `colfreqs`) be multiples of the minimum sampling frequency `minfreq`.\n",
    "\n",
    "The method of detecting the period was deviced by Yiqun, and my understanding of it is not very firm. Nevertheless, I will try to explain the gist of what I know. Details will be skipped over in favour of conveying the overall idea because the details are in the code below.\n",
    "\n",
    "First, we calculate the autocorrelation function for each time series in the data. We then add them all up and store them in `acfs`, aligning them using `minfreq`.\n",
    "\n",
    "To illustrate, if we are given signals _a_ and _b_, with sampling frequencies 5 minutes and 10 minutes respectively, `minfreq` would be 5 minutes. Suppose we calculate the autocorrelation functions for each signal and the value at lag 2 for _b_ is `0.3` while the value at lag 4 for _a_ is `0.5`. `acfs[4]` would hence hold the value `0.8`.\n",
    "\n",
    "After calculating `acfs` in this manner, we next obtain the indices of the local maxima within `acfs` and store it in `max_corr_points`. Then we take the absolute differences between every possible pair of indices in `max_corr_points`. Thus, we obtain a list of index differences between lags exhibiting high autocorrelation. The idea is that if the time series is periodic over _n_ time steps, then the autocorrelation will be high for lag _n_, _2n_, _3n_, ...\n",
    "\n",
    "If we cannot find a period, we reject the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460b5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPeriodError(Exception):\n",
    "    pass\n",
    "\n",
    "def get_period(df, colfreqs, minfreq):\n",
    "    # check that all sampling frequencies are multiples of the smallest sampling frequency\n",
    "    notime = pd.Timedelta(0)\n",
    "    for f in colfreqs.values():\n",
    "        if f % minfreq != notime:\n",
    "            raise ValueError(\"detected sampling frequency that is not a multiple of the minimum sampling frequency\")\n",
    "\n",
    "    # there is no point calculating autocorrelation for lags greater than n/2\n",
    "    nlags = len(df)//2  # [ANDY] assuming df covers at least two periods ? Why?\n",
    "    acfs = np.zeros((nlags+1,))\n",
    "    for y in colfreqs:\n",
    "        yvals = df[y].dropna()\n",
    "        yacfs = sts.acf(yvals, nlags=len(yvals)//2)\n",
    "        step_size = colfreqs[y]//minfreq\n",
    "        yacfs_idx = range(0, len(acfs), step_size)\n",
    "        # we simplify by cropping because if len(df) % len(yvals) != step_size we get tedious off-by-one errors\n",
    "        yacfs = yacfs[:len(yacfs_idx)]\n",
    "        # if y has missing values, yacfs can be shorter than yacfs_idx -_-\n",
    "        yacfs_idx = yacfs_idx[:len(yacfs)]\n",
    "        acfs[yacfs_idx] += yacfs\n",
    "\n",
    "    max_corr_points = argrelextrema(acfs, np.greater, order=max(len(df)//1000, 2))  # [ANDY] why max(len(df)//1000, 2))?\n",
    "    max_corr_points = max_corr_points[0]\n",
    "    max_corr_points = max_corr_points[acfs[max_corr_points]>0.2]  # [ANDY] why 0.2?\n",
    "    max_corr_points = np.insert(max_corr_points, 0, 0, axis=0)\n",
    "    max_cor_diff = []\n",
    "    for point_1 in max_corr_points:\n",
    "        for point_2 in max_corr_points:\n",
    "            if point_1==point_2: continue\n",
    "            max_cor_diff.append(abs(point_1-point_2))\n",
    "    max_cor_diff = np.array(max_cor_diff)\n",
    "    unique_vals, counts = np.unique(max_cor_diff, return_counts=True)\n",
    "    adjust_counts = []\n",
    "    for idx in range(len(unique_vals)):\n",
    "        adjust_counts.append(np.sum(counts[np.where(unique_vals%unique_vals[idx]==0)]))\n",
    "    if(np.max(adjust_counts)>5):  # [ANDY] why 5?\n",
    "        return unique_vals[np.argmax(adjust_counts)]\n",
    "    raise NoPeriodError(\"no period detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad131737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/statsmodels/tsa/stattools.py:541: FutureWarning: fft=True will become the default in a future version of statsmodels. To suppress this warning, explicitly set fft=False.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "period = get_period(df, colfreqs, minfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50aafdc",
   "metadata": {},
   "source": [
    "## Align data\n",
    "\n",
    "This step tries to calculate the alignments for each column, in case there are misaligned values. The basic idea is as follows. Given a time series that starts at _t_ with sampling frequency _f_, you would expect there all the data to have a time index of _t + nf_, where _n_ is some positive integer. Any sequence of values that instead fall on _t + i + nf_, where _i_ is a non-zero value, is misaligned. \n",
    "\n",
    "Our approach is to first find groups of contiguous samples where _t + nf_ is true if we take the first time index in the group as _t_. Then we merge the non-contiguous groups based on whether they are aligned with each other. We assume that the largest such merged group is the one with the correct alignment and force every other group to align with it.\n",
    "\n",
    "We then returned the aligned data as well as `zeropoint`, which is first time point of the aligned data, as well as `anchors`, which is the first time point of each column after alignment. The `anchor` and `zeropoint` values allow us to check if future CSVs belonging to the same time series are consistent with the training data. For scoring, we `align_data()` rejects the data if it is inconsistent with `anchors` after alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d704c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisalignedColumnsError(Exception):\n",
    "    pass\n",
    "\n",
    "def align_data(df, colfreqs, minfreq, anchors=None, zeropoint=None):\n",
    "    \"\"\"aligns data according to sampling frequencies\"\"\"\n",
    "    aligned_df = {}\n",
    "    for y, freq in colfreqs.items():\n",
    "        col_df = df[y].dropna()\n",
    "        index_diff = col_df.index[1:]-col_df.index[:-1]\n",
    "        sampling_groups = []\n",
    "        cur_group = [col_df.index[0]]\n",
    "        for idx in range(len(index_diff)):\n",
    "            if (index_diff[idx]==freq):\n",
    "                cur_group.append(col_df.index[idx+1])\n",
    "            else:\n",
    "                if (len(cur_group)>0):\n",
    "                    sampling_groups.append(cur_group)\n",
    "                cur_group = [cpdol_df.index[idx+1]]\n",
    "\n",
    "        if (len(cur_group)>0):\n",
    "            sampling_groups.append(cur_group)\n",
    "            \n",
    "        merged_sampling_group = []\n",
    "        while len(sampling_groups)>0:\n",
    "            cur_group = sampling_groups.pop(0)\n",
    "            merged_idx = []\n",
    "            for gidx, group in enumerate(sampling_groups):\n",
    "                if ((group[0]-cur_group[-1])%freq==0):  # [ANDY] (group[0]-cur_group[-1])%freq == pd.Timedelta(0)?\n",
    "                    cur_group.extend(group)\n",
    "                    merged_idx.append(gidx)\n",
    "        \n",
    "            for m_idx in sorted(merged_idx, reverse=True):\n",
    "                del sampling_groups[m_idx]\n",
    "            \n",
    "            merged_sampling_group.append(cur_group)\n",
    "            \n",
    "        merged_group_lens = [len(group) for group in merged_sampling_group]\n",
    "        dominant_group_idx = np.argmax(merged_group_lens)\n",
    "        init_group = merged_sampling_group[dominant_group_idx]\n",
    "        \n",
    "        n_samplings_before = round((init_group[0]-df.index[0])/freq)+1\n",
    "        n_samplings_after = round((df.index[-1] - init_group[0])/freq)  # [ANDY] should +1?\n",
    "        index_before = pd.date_range(end=init_group[0], periods = n_samplings_before, freq = freq, closed = 'left')\n",
    "        index_after = pd.date_range(start=init_group[0], periods = n_samplings_after, freq = freq)\n",
    "        aligned_index = index_before.append(index_after)\n",
    "        \n",
    "        # need to understand the mechanism of reindex on how it fill the NA value\n",
    "        aligned_col_df = col_df.reindex(index=aligned_index, method='nearest', limit=1)\n",
    "        aligned_df[y] = aligned_col_df\n",
    "        \n",
    "    aligned_df = pd.DataFrame(aligned_df)\n",
    "\n",
    "    if anchors is None:\n",
    "        anchors = {y:aligned_df[[y]].dropna().index[0] for y in colfreqs}\n",
    "        #NOTE: the zeropoint value is necessary for building the time_index column, which fill_gaps does\n",
    "        zeropoint = aligned_df.index[0]\n",
    "    #if anchors is passed in, do a sanity check on alignment\n",
    "    else:\n",
    "        notime = pd.Timedelta(seconds=0)\n",
    "        for y, yfreq in colfreqs.items():\n",
    "            thisorigin = aligned_df[[y]].dropna().index[0]\n",
    "            if (thisorigin - anchors[y]) % yfreq != notime:\n",
    "                raise MisalignedColumnsError(f\"after aligning the data, column {y}'s zero point {thisorigin} is not aligned with the recorded anchor point {anchors[y]} with sampling frequency {yfreq}\")\n",
    "\n",
    "    return aligned_df, anchors, zeropoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b3b7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, anchors, zeropoint = align_data(df, colfreqs, minfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ced66a",
   "metadata": {},
   "source": [
    "## Drop extremes\n",
    "\n",
    "Note that the period returned by `get_period()` is a positive integer. The actual period of the data in units of time is `period * minfreq`. For training, want to drop outlier values. Implementation details are in the code, but the basic idea is we label every row in `df` as a value within `range(0, period)`. Then for each column and each such label value, we calculate the mean and standard deviation of all the rows with that label, and those rows with values that exceed 2 standard deviations from the mean for that label are set to null.\n",
    "\n",
    "### Example\n",
    "\n",
    "Dataframe (period is _3_, sampling frequency is _f_, v is just a placeholder):\n",
    "\n",
    "| time | value |\n",
    "| --- | --- |\n",
    "| _t_ | _v_ |\n",
    "| _t+f_ | _v_ |\n",
    "| t+2f | _v_ |\n",
    "| _t+3f_ | _v_ |\n",
    "| _t+4f_ | _v_ |\n",
    "| t+5f | _v_ |\n",
    "\n",
    "Dataframe with time index:\n",
    "\n",
    "| time | value | time_index |\n",
    "| --- | --- | --- |\n",
    "| _t_ | _v_ | 0 |\n",
    "| _t+f_ | _v_ | 1 |\n",
    "| t+2f | _v_ | 2 |\n",
    "| _t+3f_ | _v_ | 0 |\n",
    "| _t+4f_ | _v_ | 1 |\n",
    "| t+5f | _v_ | 2 |\n",
    "\n",
    "In the example above, when deciding whether to set _v_ to null for _t+f_, we would consider the mean and standard deviation for the rows _t+f_ and _t+4f_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d12966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_extremes(df, freq, period, ycols):  # [ANDY] confirmed ts has been aligned, no need to fill ts gaps in this func again\n",
    "    \"\"\"drop values beyond 2 standard deviations from the mean\"\"\"\n",
    "    # calculate the points where the gap is more than n_max_fill*interval\n",
    "    time_between_points = df.index.to_series().diff()\n",
    "    gaps = time_between_points[(time_between_points>=2*freq)]\n",
    "\n",
    "    index_all = df.index\n",
    "    for gidx, gap in gaps.iteritems():\n",
    "        gap_length = math.ceil(gap/freq)\n",
    "        padding = pd.date_range(end = gidx, periods = gap_length, freq = freq, closed = 'left')\n",
    "        index_all = index_all.append(padding)\n",
    "\n",
    "    index_all = index_all.sort_values()\n",
    "    df = df.reindex(index_all)  # [ANDY] df is already aligned in last step, why there are still ts gaps to be filled?\n",
    "\n",
    "    time_index = [i for i in range(0, period)]\n",
    "    data_time_index = np.array([i % period for i in range(0, len(df))])\n",
    "    data_time_index_series = pd.Series(index=index_all, data=data_time_index)\n",
    "\n",
    "    n_std = 2\n",
    "\n",
    "    for y in ycols:\n",
    "        for period_idx in time_index:\n",
    "            lbound = df[y][data_time_index_series==period_idx].mean()-n_std*df[y][data_time_index_series==period_idx].std()\n",
    "            ubound = df[y][data_time_index_series==period_idx].mean()+n_std*df[y][data_time_index_series==period_idx].std()\n",
    "        \n",
    "            sub_data = df[y][data_time_index_series==period_idx]\n",
    "            if (len(sub_data[(sub_data<lbound) | (sub_data>ubound)].index)>0):\n",
    "                df.loc[sub_data[(sub_data<lbound) | (sub_data>ubound)].index, y] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8feaa285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_extremes(df, minfreq, period, ycols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b864c",
   "metadata": {},
   "source": [
    "## Normalisation parameters\n",
    "\n",
    "We did not previously normalise the data, but we noticed that columns with a larger range of values had a greater effect on anomaly scores, so we decided to make sure the values in every column fall between 0 and 1. Hence, we record the minimum and maximum values of the training data for each column. We reuse these mininum and maximum values when normalising future batches of data for scoring, because it is fine for scoring data to exceed these bounds so long as they are normalised with the same parameters.\n",
    "\n",
    "Here we also add a `time_index` column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d8d553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model normalisation params\n",
    "modnorms = {y:( df[y].min(), df[y].max() ) for y in ycols}\n",
    "df['time_index'] = [((t-zeropoint)//minfreq)%period for t in df.index]  # [ANDY] period is matched to minfreq? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8b3e3",
   "metadata": {},
   "source": [
    "## Get median profiles\n",
    "\n",
    "We calculate and store the median values for each column and time index. This information is used for filling gaps and also when scoring. This time index thing is calculated multiple times in different places. Sorry about that. -_-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97748cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_profiles(df, ycols, period):  # [ANDY] get median profiles after dropping extreme values?\n",
    "    time_index = list(range(period))\n",
    "\n",
    "    median_profiles = pd.DataFrame(index=time_index)\n",
    "\n",
    "    for y in ycols:\n",
    "        median_profiles[y] = [df[df['time_index']==t][y].median() for t in time_index]\n",
    "\n",
    "    return median_profiles, time_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee3b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_profiles, time_index = get_median_profiles(df, ycols, period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8098077",
   "metadata": {},
   "source": [
    "## Fill gaps\n",
    "\n",
    "Since the input for the autoencoder has to be of fixed dimensions, we need to fill all the gaps in the data. We do this column by column. For small gaps, we do linear interpolation. For larger gaps, we use the median profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ea425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df, colfreqs, minfreq, period, median_profiles, zeropoint):\n",
    "    df = df.copy()\n",
    "\n",
    "    firstts = df.index[0]\n",
    "    lastts = df.index[-1]\n",
    "    for y, yfreq in colfreqs.items():\n",
    "        yfactor = yfreq//minfreq\n",
    "        yperiod = period//yfactor\n",
    "        ydf = df[[y]].dropna()  # [ANDY] df already aligned.. if don't dropna, df[[y]] is already alighed with correct ts, just need to fill value gaps instead of ts gaps\n",
    "        ydf, gaps = gap_reindex(ydf, yfreq, firstts, lastts)  # [ANDY] why still need to fill index(ts) gaps?\n",
    "        ydf['time_index'] = [((t-zeropoint)//minfreq)%period for t in ydf.index]\n",
    "        ydf = fill_gaps_col(ydf, y, gaps, yfreq, yperiod, median_profiles)\n",
    "        newindex = ydf.index.union(df.index).sort_values()  # [ANDY] redundant, index is already aligned\n",
    "        df = df.reindex(newindex)  # [ANDY] redundant, index is already aligned\n",
    "        df[y] = ydf[y]\n",
    "\n",
    "    df['time_index'] = [((t-zeropoint)//minfreq)%period for t in df.index]\n",
    "    return df\n",
    "\n",
    "def gap_reindex(df, freq, firstts, lastts):\n",
    "    # calculate the points where the gap is more than n_max_fill*freq\n",
    "    # these points don't want to interpolate\n",
    "    time_between_points = df.index.to_series().diff()\n",
    "\n",
    "    # if first timestamp for this column is not equal to first timestamp for all columns, we need to fill\n",
    "    yfirstts = df.index[0]\n",
    "    if yfirstts!=firstts:\n",
    "        time_between_points.at[yfirstts] = yfirstts - firstts + freq\n",
    "\n",
    "    # if last timestamp for this column is not equal to last timestamp for all columns, we need to fill\n",
    "    ylastts = df.index[-1]\n",
    "    if ylastts!=lastts:\n",
    "        time_between_points.at[lastts+freq] = lastts - ylastts + freq\n",
    "\n",
    "    gaps = time_between_points[(time_between_points>=2*freq)]\n",
    "\n",
    "    index_all = df.index\n",
    "    padded_indices = pd.date_range(end = min(index_all), periods = 0, freq = freq, closed = 'left')  # [ANDY] why this?\n",
    "    # create index positions for gaps\n",
    "    for gidx, gap in gaps.iteritems():\n",
    "        gap_length = math.ceil(gap/freq)\n",
    "        padding = pd.date_range(end = gidx, periods = gap_length, freq = freq, closed = 'left')\n",
    "        padded_indices = padded_indices.union(padding)  # [ANDY] why this?\n",
    "        index_all = index_all.append(padding)\n",
    "\n",
    "    index_all = index_all.sort_values()\n",
    "\n",
    "    df = df.reindex(index_all)\n",
    "    return df, gaps\n",
    "\n",
    "def fill_gaps_col(df, y, gaps, freq, period, median_profiles):\n",
    "    # interpolate large gaps using median values\n",
    "    max_miss_length = period//10\n",
    "    for gidx, gap in gaps.iteritems():\n",
    "        if (gap>max_miss_length*freq):\n",
    "            gap_length = math.ceil(gap/freq)\n",
    "            n_periods = 2\n",
    "            while (n_periods<=gap_length):\n",
    "                pre_index = pd.date_range(end = gidx, periods = n_periods, freq = freq, closed = 'left')\n",
    "                pre_index = pre_index[0]\n",
    "\n",
    "                df.loc[pre_index, y] = median_profiles.loc[df.loc[pre_index, \"time_index\"], y]\n",
    "\n",
    "                n_periods = n_periods+1\n",
    "\n",
    "    # small gaps interpolated linearly\n",
    "    return df.interpolate(method='linear', limit_direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9092c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_gaps(df, colfreqs, minfreq, period, median_profiles, zeropoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362a21b",
   "metadata": {},
   "source": [
    "## Model metadata\n",
    "\n",
    "This part is a bit of engineering detail. It is used for persisting some of the metadata required to create the model. Some names are weird because of the legacy of having to train multiple autoencoders previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be74758",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time_index[-1]\n",
    "t_to_fname = {t: f\"model-params.h5\"}\n",
    "inputsize = sum(period//(freq//minfreq) for freq in colfreqs.values())\n",
    "modelmeta = {\"median_profiles\": median_profiles, \"input_size\":inputsize, \"t_to_fname\": t_to_fname, \"modnorms\": modnorms}\n",
    "modfname = t_to_fname[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481d22e",
   "metadata": {},
   "source": [
    "## Get full periods of data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "208c3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get times within df corresponding to t in period that can yield a full period of training data\n",
    "# i.e. time - period >= df.index[0]\n",
    "fullperiods = df.iloc[period-1:]  # [ANDY] why? should be period-1\n",
    "times = fullperiods[fullperiods['time_index']==t].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b1405",
   "metadata": {},
   "source": [
    "## Generate model input\n",
    "\n",
    "We use the same function to generate model inputs for both training and scoring.\n",
    "\n",
    "### Shifting the data\n",
    "\n",
    "Originally, we used to train `period` number of autoencoders for each set of data, because we thought we needed to remember `period` number of patterns. Basically, if the period is 3, then every point in the data is labelled [0, 1, 2, 0, 1, 2, ...]. Hence there are 3 different patterns we needed to remember. The one with all the windows of data from [0,1,2], the one with all the windows of data from [1,2,0], and the one with all the windows of data from [2,0,1]. Later, we realised that we were actually training a bunch of different models that remember the same pattern, just at different phases of the period. So we started to train only one model that remembered the pattern of data for the window from [0,1,2]. When we needed to score data for a [2,0,1] window, we just left shifted it by one position before sending it to the autoencoder. To see how we do this, see how `startpos` is calculated and used in the code below.\n",
    "\n",
    "### Normalise the data\n",
    "\n",
    "This is fairly straightforward.\n",
    "\n",
    "### Handling multiple columns\n",
    "\n",
    "To handle multiple columns with a single autoencoder, we just concatenate all the columns. As long as the relative positions of the columns are preserved and the input dataframe has already been aligned, we can always unpack the autoencoder output in the same way. The autoencoder has no concept of different columns in any case.\n",
    "\n",
    "### Differences between training and scoring\n",
    "\n",
    "We train the model with the data in `df` as is.\n",
    "\n",
    "For scoring, we pass in `median_values`, which is the `median_profiles` we calculated earlier. Supposing the period and therefore width of the input to the model is 100, we basically only pass the last 10 points of real data into the model and replace the rest of the window with the median values for each point in the period. Hence when we generate the anomaly scores for the window ending at point _t_, we are basically asking the model the following question.\n",
    "\n",
    "Given the assumption that points _t-99 - t-10_ are normal, what is the chance that _t-9 - t_ is abnormal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38e36170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normdf(df, modnorms):\n",
    "    normed = df.copy()\n",
    "    for y, ( ymin, ymax ) in modnorms.items():\n",
    "        mmdiff = ymax - ymin\n",
    "        normed[y] = (normed[y]-ymin)/mmdiff\n",
    "    return normed\n",
    "\n",
    "def generate_model_input(df, ycols, times, period, modnorms, offset=None, median_values=None):\n",
    "    \"\"\"\n",
    "generate model input for windows ending at timestamps in the times arguments\n",
    "\n",
    "Assumptions:\n",
    "    1. times all have the same time index relative to period\n",
    "    2. sampling intervals for each column are regular\n",
    "    \"\"\"\n",
    "    offset = period-1 if offset is None else offset\n",
    "    window_size = 10\n",
    "    rows = []\n",
    "    for t in times:\n",
    "        idx = df.index.get_loc(t)+1\n",
    "        rowdf = df.iloc[idx-period:idx].copy()\n",
    "        if median_values is not None and window_size<period:\n",
    "            replaceidx = rowdf.iloc[:len(rowdf)-window_size].index\n",
    "            rowdf.loc[replaceidx, ycols] = median_values.loc[rowdf.loc[replaceidx, \"time_index\"], ycols].to_numpy()\n",
    "        # normalize training/scoring data\n",
    "        rowdf = normdf(rowdf, modnorms)\n",
    "        startpos = period-1-offset\n",
    "        rowdf = rowdf.iloc[startpos:].append(rowdf.iloc[:startpos])  # [ANDY] why?\n",
    "        row = np.concatenate([rowdf[y].dropna() for y in ycols])\n",
    "        rows.append(row)\n",
    "\n",
    "    collens = set(len(r) for r in rows)\n",
    "    if len(collens)!=1:\n",
    "        logging.error(f\"detected row lengths: {collens}\")\n",
    "        raise MisalignedColumnsError(\"unable to create input data because dimensions don't match\")\n",
    "\n",
    "    numcols = collens.pop()\n",
    "\n",
    "    return np.concatenate(rows).reshape(( -1, numcols ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "989d03e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 48)\n",
      "2880\n"
     ]
    }
   ],
   "source": [
    "dat = generate_model_input(df, ycols, times, period, modnorms)\n",
    "print(dat.shape)\n",
    "print(dat.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e2c36",
   "metadata": {},
   "source": [
    "## Test for pipeline\n",
    "Call a pipeline class to execute preprocessing functions one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb36eb42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period_detect正在检查\n",
      "align_data正在检查\n",
      "drop_extrema正在检查\n",
      "normalizer正在检查\n",
      "fill_gap正在检查\n",
      "generate_input正在检查\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#test for pipeline\n",
    "from preprocessing._preprocessor import LoadData, FindFrequency, PeriodDetect, AlignData, DropExtrema, Normalizer, FillGap, GenerateInput\n",
    "from pipeline import Pipeline\n",
    "\n",
    "test_task_list = [\n",
    "    LoadData(),\n",
    "    FindFrequency(),\n",
    "    PeriodDetect(),\n",
    "    AlignData(),\n",
    "    DropExtrema(),\n",
    "    Normalizer(),\n",
    "    FillGap(),\n",
    "    GenerateInput()\n",
    "]\n",
    "test_pipeline = Pipeline(test_task_list)\n",
    "dat3, params3 = test_pipeline.fit_transform()\n",
    "\n",
    "print(dat == dat3)\n",
    "colfreqs3 = params3['colfreqs']\n",
    "is_equal = colfreqs3 == colfreqs\n",
    "print(is_equal)\n",
    "print(minfreq == params3['minfreq'])\n",
    "print(period == params3['period'])\n",
    "print(zeropoint == params3['zeropoint'])\n",
    "print(anchors == params3['anchors'])\n",
    "print(modnorms == params3['modnorms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5e81c",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "I am assuming the person who reads this will be more familiar with this part than I am, so I am not going to say much here. It is a just a basic autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07974a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, weight_filename):\n",
    "    timelimit = 5400 # 1.5 hours\n",
    "    starttime = time.time()\n",
    "\n",
    "    not_overtime = lambda t: t-starttime<timelimit\n",
    "\n",
    "    logend_cb = LogEndCallback()\n",
    "    n_inputs = data.shape[1]\n",
    "\n",
    "    # we record 30 errors for averaging. if average of 30 errors is not decreasing,\n",
    "    # we stop training\n",
    "    ave_step = 30\n",
    "    past_err_store = np.array([0.0 for i in range(ave_step)])\n",
    "    nextpos = logend_cb.runs % ave_step\n",
    "    prev_average_err = None\n",
    "\n",
    "    model = construct_ae_model(n_inputs)\n",
    "    model = learn_ae_model(model, data, logend_cb)\n",
    "    \n",
    "    test_error = np.sum(np.square(data - model.predict(data)))\n",
    "    sumsquare = np.sum(np.square(data))\n",
    "\n",
    "    while (test_error/sumsquare>0.001) and not_overtime(time.time()):  # [ANDY] why 0.001? not consistent with the scale factor used in defining early stoppping baseline\n",
    "        print(logend_cb.runs)\n",
    "        nextpos = logend_cb.runs % ave_step\n",
    "        model = learn_ae_model(model, data, logend_cb)\n",
    "        \n",
    "        test_error = np.sum(np.square(data - model.predict(data)))\n",
    "        if logend_cb.runs % 10 == 0:\n",
    "            logging.info(f\"runs = {logend_cb.runs}, sumsquare = {sumsquare}, test_error = {test_error} for {weight_filename}.\")\n",
    "\n",
    "        past_err_store[nextpos] = test_error\n",
    "        if nextpos == ave_step-1:\n",
    "            average_err = past_err_store.mean()\n",
    "            if prev_average_err is not None and average_err>=prev_average_err: \n",
    "                logging.info(f\"stopping because previous average error {prev_average_err} is smaller than current average error {average_err}\")\n",
    "                break\n",
    "            if prev_average_err is not None:\n",
    "                logging.info(f\"training {weight_filename} errs: {past_err_store}\\naverage of old errors: {prev_average_err}. average of new errors: {average_err}\")\n",
    "            prev_average_err = average_err\n",
    "\n",
    "    logging.info(\"saving {}\".format(weight_filename))\n",
    "    model.save_weights(weight_filename)\n",
    "    K.clear_session()\n",
    "\n",
    "class LogEndCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.runs = 0\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.runs += 1\n",
    "        \n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                logging.debug(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def construct_ae_model(n_inputs):\n",
    "    n_units_1 = math.floor(n_inputs/2)#max(math.floor(n_inputs/4), 30)\n",
    "    n_units_2 = math.floor(n_units_1/2)#max(math.floor(n_units_1/4), 15)\n",
    "    # define model\n",
    "    ae_model = Sequential()\n",
    "    ae_model.add(Dense(n_units_1, activation='relu', input_shape=(n_inputs,)))\n",
    "    ae_model.add(Dense(n_units_2, activation='relu', input_shape=(n_units_1,)))\n",
    "    ae_model.add(Dense(n_units_1, activation='relu', input_shape=(n_units_2,)))\n",
    "    ae_model.add(Dense(n_inputs, activation='linear'))\n",
    "    ae_model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    \n",
    "    return ae_model\n",
    "\n",
    "def learn_ae_model(ae_model, data, logend_cb):\n",
    "    batch_size = 32\n",
    "    #mean_norm = np.mean(np.square(data.values.reshape(data.shape[0], -1)))\n",
    "    mean_norm = np.mean(np.square(data))\n",
    "    baseline = mean_norm/10000.0\n",
    "    logging.debug('baseline=' + str(baseline))\n",
    "    \n",
    "    # fit model\n",
    "    loss_cb = EarlyStoppingByLossVal(monitor='val_loss', value=baseline)\n",
    "    ae_model.fit(data, data, validation_data=(data, data), epochs=200, callbacks=[loss_cb, logend_cb], batch_size=batch_size, shuffle=True, verbose=0)\n",
    "    \n",
    "    return ae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2005918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/preh0405/opt/anaconda3/envs/chrono/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:saving model-params.h5\n"
     ]
    }
   ],
   "source": [
    "train_model(dat, modfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8231c53",
   "metadata": {},
   "source": [
    "## New training model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4e451f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.dataset import random_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0f8dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AE structure\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, first_layer, layer_1, layer_2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(first_layer, layer_1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(layer_1,layer_2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(layer_2, layer_1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(layer_1, first_layer)            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_encoder = self.encoder(x)\n",
    "        x_decoder = self.decoder(x_encoder)\n",
    "\n",
    "        return x_encoder,x_decoder\n",
    "    \n",
    "#custom dataset\n",
    "class ChronosDataset(Dataset):\n",
    "    def __init__(self,dat):\n",
    "        self.dat = dat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.dat[idx]\n",
    "\n",
    "#Trainning function\n",
    "def _fit(ae, train_loader, val_loader, epochs, batch_size, tolerence, optimizer, scheduler, loss_f, writer):\n",
    "    best_model = ae\n",
    "    best_loss = 0.0\n",
    "    patience_level = 0\n",
    "    timelimit = 900 # 15 mins maximum for each tolerence level\n",
    "    starttime = time.time()\n",
    "\n",
    "    not_overtime = lambda t: t-starttime<timelimit\n",
    "    #for epoch in range(epochs):\n",
    "    epoch = 0\n",
    "    while not_overtime(time.time()):\n",
    "        average_loss = 0.0\n",
    "        for batchidx, x in enumerate(train_loader):\n",
    "            x = Variable(x.float())\n",
    "            _x_encoded, _x_decoded = ae(x.float())\n",
    "            loss = loss_f(_x_decoded,x)\n",
    "            writer.add_scalar(f'loss_{tolerence}',loss, epoch * batch_size + batchidx)\n",
    "            average_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        average_loss = average_loss / batch_size\n",
    "        \n",
    "        #Validation & Checking Early Stopping \n",
    "        with torch.no_grad():\n",
    "            current_loss = 0.0\n",
    "            for batchidx, x in enumerate(val_loader):\n",
    "                ae.eval()\n",
    "                _x_encoded, _x_decoded = ae(x.float())\n",
    "                val_loss = loss_f(_x_decoded,x)\n",
    "                current_loss += val_loss\n",
    "                writer.add_scalar(f'val_loss_{tolerence}',val_loss, epoch * batch_size + batchidx)\n",
    "            current_loss = current_loss / batch_size\n",
    "            if epoch == 0:\n",
    "                best_loss = current_loss        \n",
    "            elif current_loss < best_loss:\n",
    "                patience_level = 0\n",
    "                best_model = ae\n",
    "                best_loss = current_loss\n",
    "            else:\n",
    "                patience_level += 1\n",
    "        \n",
    "        if epoch % 10000 == 0:\n",
    "            print(f'epoch number:{epoch} train_loss: {average_loss} val_loss{current_loss}')\n",
    "        scheduler.step(average_loss)\n",
    "        \n",
    "        if patience_level >= tolerence:\n",
    "            logging.info(f\"Early stopping is triggered, current best loss: {best_loss}\")\n",
    "            break\n",
    "            \n",
    "        epoch += 1\n",
    "#     torch.save({'epoch': epoch, 'state_dict': best_model.state_dict(), 'best_loss': best_loss,\n",
    "#                             'optimizer': optimizer.state_dict()},\n",
    "#                            f'temperary_model/temp_model_{tolerence}.pth.tar')       \n",
    "    return best_model, best_loss\n",
    "\n",
    "# def load_checkpoint(model, checkpoint_PATH, optimizer):\n",
    "#     if checkpoint != None:\n",
    "#         model_CKPT = torch.load(checkpoint_PATH)\n",
    "#         model.load_state_dict(model_CKPT['state_dict'])\n",
    "#         print('loading checkpoint!')\n",
    "#         optimizer.load_state_dict(model_CKPT['optimizer'])\n",
    "#     return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa29176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:0 train_loss: 0.048693567514419556 val_loss0.024267058582542436\n",
      "Epoch   804: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch   930: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch  1056: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch  1182: reducing learning rate of group 0 to 1.0000e-05.\n",
      "epoch number:10000 train_loss: 4.63717442471534e-05 val_loss2.3787735887854198e-05\n",
      "epoch number:20000 train_loss: 4.758046998176724e-05 val_loss2.126538657895949e-05\n",
      "epoch number:30000 train_loss: 2.480033435858786e-05 val_loss1.535778048021139e-05\n",
      "epoch number:40000 train_loss: 1.680769128142856e-05 val_loss1.1460278310941481e-05\n",
      "epoch number:50000 train_loss: 1.0653895515133627e-05 val_loss4.356602725053299e-06\n",
      "epoch number:60000 train_loss: 6.823782314313576e-06 val_loss2.1187335290708904e-06\n",
      "epoch number:70000 train_loss: 8.158544915204402e-06 val_loss1.8915509541670686e-06\n",
      "epoch number:80000 train_loss: 7.6007227107766084e-06 val_loss1.8250015956453977e-06\n",
      "epoch number:90000 train_loss: 7.172207006078679e-06 val_loss1.7764319221241945e-06\n",
      "epoch number:100000 train_loss: 8.479823009110987e-06 val_loss1.7237670952742609e-06\n",
      "epoch number:110000 train_loss: 5.308025720296428e-06 val_loss1.6863443214705977e-06\n",
      "epoch number:120000 train_loss: 6.502330961666303e-06 val_loss1.634790048086989e-06\n",
      "epoch number:130000 train_loss: 4.37843755207723e-06 val_loss1.5772973883594782e-06\n",
      "epoch number:140000 train_loss: 4.519782578427112e-06 val_loss1.5747712450100237e-06\n",
      "epoch number:150000 train_loss: 6.0517977544805035e-06 val_loss1.5670885687296332e-06\n",
      "epoch number:160000 train_loss: 5.825620519317454e-06 val_loss1.5635899579491729e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Early stopping is triggered, current best loss: 1.5217063155175392e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:0 train_loss: 4.351084044174058e-06 val_loss1.5417144042839605e-06\n",
      "epoch number:10000 train_loss: 4.187503691355232e-06 val_loss1.5416200586240034e-06\n",
      "epoch number:20000 train_loss: 3.7815364066773327e-06 val_loss1.5219664422065179e-06\n",
      "epoch number:30000 train_loss: 3.397030013729818e-06 val_loss1.4495171309908025e-06\n",
      "epoch number:40000 train_loss: 3.462730091996491e-06 val_loss1.3517855996601056e-06\n",
      "epoch number:50000 train_loss: 2.15887803278747e-06 val_loss1.23484875525189e-06\n",
      "epoch number:60000 train_loss: 1.7856057183962548e-06 val_loss1.19584646983955e-06\n",
      "epoch number:70000 train_loss: 1.7353402199660195e-06 val_loss1.1848037750310326e-06\n",
      "epoch number:80000 train_loss: 1.734080569804064e-06 val_loss1.1840047470954328e-06\n",
      "epoch number:90000 train_loss: 1.6830963431857526e-06 val_loss1.1831912338083325e-06\n",
      "epoch number:100000 train_loss: 1.7260989579881425e-06 val_loss1.178681079702608e-06\n",
      "epoch number:110000 train_loss: 1.8141353166356566e-06 val_loss1.1767048779652261e-06\n",
      "epoch number:120000 train_loss: 1.7255949842365226e-06 val_loss1.1703881413412555e-06\n",
      "epoch number:130000 train_loss: 1.6398394109273795e-06 val_loss1.1696619538039261e-06\n",
      "epoch number:140000 train_loss: 1.746851467032684e-06 val_loss1.1673127972326446e-06\n",
      "epoch number:150000 train_loss: 1.6830306321935495e-06 val_loss1.1622965775275351e-06\n",
      "epoch number:160000 train_loss: 1.608944785402855e-06 val_loss1.1630487160616295e-06\n",
      "epoch number:170000 train_loss: 1.651458092055691e-06 val_loss1.1560634672078176e-06\n",
      "epoch number:180000 train_loss: 1.7153001863334794e-06 val_loss1.1554993556163815e-06\n",
      "epoch number:190000 train_loss: 1.6764608972152928e-06 val_loss1.1570855370140036e-06\n",
      "epoch number:200000 train_loss: 1.5681637250963831e-06 val_loss1.154235269486281e-06\n",
      "epoch number:210000 train_loss: 1.6841393062350107e-06 val_loss1.1502353048893803e-06\n",
      "epoch number:220000 train_loss: 1.6393560144933872e-06 val_loss1.14851330959965e-06\n",
      "epoch number:230000 train_loss: 1.4516721194013371e-06 val_loss1.1435489984689179e-06\n",
      "epoch number:240000 train_loss: 1.6508845419593854e-06 val_loss1.1447676426485305e-06\n",
      "epoch number:250000 train_loss: 1.6327835510310251e-06 val_loss1.141353543382912e-06\n",
      "epoch number:260000 train_loss: 1.681913090578746e-06 val_loss1.1383788092486292e-06\n",
      "epoch number:0 train_loss: 1.6209648947551614e-06 val_loss1.1364108519972014e-06\n",
      "epoch number:10000 train_loss: 1.6002704796846956e-06 val_loss1.1348319070128496e-06\n",
      "epoch number:20000 train_loss: 1.6767548913776409e-06 val_loss1.130369792450231e-06\n",
      "epoch number:30000 train_loss: 1.4344456076287315e-06 val_loss1.1301978353414026e-06\n",
      "epoch number:40000 train_loss: 1.6522244550287724e-06 val_loss1.127300687934865e-06\n",
      "epoch number:50000 train_loss: 1.5268917650246294e-06 val_loss1.1247847910588108e-06\n",
      "epoch number:60000 train_loss: 1.5412736047437647e-06 val_loss1.123496675214098e-06\n",
      "epoch number:70000 train_loss: 1.5061054909892846e-06 val_loss1.121219092493625e-06\n",
      "epoch number:80000 train_loss: 1.5837033515708754e-06 val_loss1.1149153400613869e-06\n",
      "epoch number:90000 train_loss: 1.4760098565602675e-06 val_loss1.1153403322904077e-06\n",
      "epoch number:100000 train_loss: 1.5267075923475204e-06 val_loss1.1081757528197263e-06\n",
      "epoch number:110000 train_loss: 1.4345048384711845e-06 val_loss1.1095626441760466e-06\n",
      "epoch number:120000 train_loss: 1.5078320529937628e-06 val_loss1.1039978429043573e-06\n",
      "epoch number:130000 train_loss: 1.5294203876692336e-06 val_loss1.1068864062693016e-06\n",
      "epoch number:140000 train_loss: 1.5100010841706535e-06 val_loss1.1022788625684682e-06\n",
      "epoch number:150000 train_loss: 1.5285015706467675e-06 val_loss1.0999490875500057e-06\n",
      "epoch number:160000 train_loss: 1.3794929145660717e-06 val_loss1.0978812186581236e-06\n",
      "epoch number:170000 train_loss: 1.4482504866464296e-06 val_loss1.0950298211873213e-06\n",
      "epoch number:180000 train_loss: 1.46845866311196e-06 val_loss1.0893725898696448e-06\n",
      "epoch number:190000 train_loss: 1.4578032505596639e-06 val_loss1.088542896122391e-06\n",
      "epoch number:200000 train_loss: 1.3852275060344255e-06 val_loss1.0925995562152709e-06\n",
      "epoch number:210000 train_loss: 1.5168878917393158e-06 val_loss1.0803926711247896e-06\n",
      "epoch number:220000 train_loss: 1.5434129636560101e-06 val_loss1.0811518120923948e-06\n",
      "epoch number:230000 train_loss: 1.3859255432180362e-06 val_loss1.0786412013440684e-06\n",
      "epoch number:240000 train_loss: 1.4427346286538523e-06 val_loss1.0715426226549624e-06\n",
      "epoch number:250000 train_loss: 1.48673780131503e-06 val_loss1.0755115145426836e-06\n",
      "epoch number:260000 train_loss: 1.5196728782029822e-06 val_loss1.0762693697490206e-06\n",
      "epoch number:0 train_loss: 1.4273027773015201e-06 val_loss1.0679963256872333e-06\n",
      "epoch number:10000 train_loss: 1.490555405325722e-06 val_loss1.0680856673058763e-06\n",
      "epoch number:20000 train_loss: 1.3656876944878604e-06 val_loss1.0639818049849468e-06\n",
      "epoch number:30000 train_loss: 1.4029410522198305e-06 val_loss1.059455495638439e-06\n",
      "epoch number:40000 train_loss: 1.4663189631392015e-06 val_loss1.0575118948908277e-06\n",
      "epoch number:50000 train_loss: 1.379888090013992e-06 val_loss1.055654581926414e-06\n",
      "epoch number:60000 train_loss: 1.4087943327467656e-06 val_loss1.0557241960143841e-06\n",
      "epoch number:70000 train_loss: 1.4532906789099798e-06 val_loss1.0451057187189111e-06\n",
      "epoch number:80000 train_loss: 1.3596770713775186e-06 val_loss1.0471250232662444e-06\n",
      "epoch number:90000 train_loss: 1.3049636891082628e-06 val_loss1.0437386577798863e-06\n",
      "epoch number:100000 train_loss: 1.3902862292525242e-06 val_loss1.040089381584592e-06\n",
      "epoch number:110000 train_loss: 1.3520600532501703e-06 val_loss1.0368211691403193e-06\n",
      "epoch number:120000 train_loss: 1.429580379408435e-06 val_loss1.0328287007827593e-06\n",
      "epoch number:130000 train_loss: 1.4600989288737765e-06 val_loss1.028792844986501e-06\n",
      "epoch number:140000 train_loss: 1.4682145774713717e-06 val_loss1.0311435014417576e-06\n",
      "epoch number:150000 train_loss: 1.3439016584015917e-06 val_loss1.0291747921277214e-06\n",
      "epoch number:160000 train_loss: 1.3361902801989345e-06 val_loss1.0269400632983136e-06\n",
      "epoch number:170000 train_loss: 1.3455743328449898e-06 val_loss1.0217423667430915e-06\n",
      "epoch number:180000 train_loss: 1.3259685829325463e-06 val_loss1.0188136378595221e-06\n",
      "epoch number:190000 train_loss: 1.4022501773069962e-06 val_loss1.0147252064746817e-06\n",
      "epoch number:200000 train_loss: 1.3963633591629332e-06 val_loss1.0190497555659386e-06\n",
      "epoch number:210000 train_loss: 1.3261354752103216e-06 val_loss1.0152547577375473e-06\n",
      "epoch number:220000 train_loss: 1.3882846587875974e-06 val_loss1.0113998099991268e-06\n",
      "epoch number:230000 train_loss: 1.4019760783412494e-06 val_loss1.0099953560919022e-06\n",
      "epoch number:240000 train_loss: 1.3491824120137608e-06 val_loss1.0015045246410673e-06\n",
      "epoch number:250000 train_loss: 1.2841608167946106e-06 val_loss1.0064049951633872e-06\n",
      "epoch number:260000 train_loss: 1.283294864151685e-06 val_loss9.998681697365275e-07\n",
      "epoch number:270000 train_loss: 1.336232116955216e-06 val_loss1.000138550848488e-06\n",
      "epoch number:0 train_loss: 1.4149838989396812e-06 val_loss1.0004142115711865e-06\n",
      "epoch number:10000 train_loss: 1.3711817246075952e-06 val_loss1.0007315393320242e-06\n",
      "epoch number:20000 train_loss: 1.2961065749550471e-06 val_loss9.965897129251979e-07\n",
      "epoch number:30000 train_loss: 1.3734352251049131e-06 val_loss9.998801404125272e-07\n",
      "epoch number:40000 train_loss: 1.2137652447563596e-06 val_loss9.970824812554548e-07\n",
      "epoch number:50000 train_loss: 1.3277747257234296e-06 val_loss9.967951392209568e-07\n",
      "epoch number:60000 train_loss: 1.3270420140543138e-06 val_loss9.967010156143412e-07\n",
      "epoch number:70000 train_loss: 1.2784920500052976e-06 val_loss9.933647207050097e-07\n",
      "epoch number:80000 train_loss: 1.3243654848338338e-06 val_loss9.925006138471348e-07\n",
      "epoch number:90000 train_loss: 1.1872150480485288e-06 val_loss9.940090148240782e-07\n",
      "epoch number:100000 train_loss: 1.3120284165779594e-06 val_loss9.897166815463198e-07\n",
      "epoch number:110000 train_loss: 1.2154932846897282e-06 val_loss9.86625950121239e-07\n",
      "epoch number:120000 train_loss: 1.2665043414017418e-06 val_loss9.933603956607815e-07\n",
      "epoch number:130000 train_loss: 1.2471111858758377e-06 val_loss9.894353585279124e-07\n",
      "epoch number:140000 train_loss: 1.270370830752654e-06 val_loss9.930169582975184e-07\n",
      "epoch number:150000 train_loss: 1.3011543842367246e-06 val_loss9.899861904959161e-07\n",
      "epoch number:160000 train_loss: 1.2266082194400951e-06 val_loss9.905741257022741e-07\n",
      "epoch number:170000 train_loss: 1.2301177321205614e-06 val_loss9.914855390200075e-07\n",
      "epoch number:180000 train_loss: 1.1884444575116504e-06 val_loss9.899865905439405e-07\n",
      "epoch number:190000 train_loss: 1.165713683803915e-06 val_loss9.88954602857968e-07\n",
      "epoch number:200000 train_loss: 1.2339786508164252e-06 val_loss9.88464473423613e-07\n",
      "epoch number:210000 train_loss: 1.1623242244240828e-06 val_loss9.885225591266716e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Early stopping is triggered, current best loss: 9.798718086711723e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:0 train_loss: 1.1876433063662262e-06 val_loss9.91039537597138e-07\n",
      "epoch number:10000 train_loss: 1.2420873645169195e-06 val_loss9.852486436421694e-07\n",
      "epoch number:20000 train_loss: 1.176393652713159e-06 val_loss9.88798121141047e-07\n",
      "epoch number:30000 train_loss: 1.2286427590879612e-06 val_loss9.866558910441463e-07\n",
      "epoch number:40000 train_loss: 1.2090081327187363e-06 val_loss9.862142942216644e-07\n",
      "epoch number:50000 train_loss: 1.1743450158974156e-06 val_loss9.858859832487914e-07\n",
      "epoch number:60000 train_loss: 1.1112637139376602e-06 val_loss9.900694688644954e-07\n",
      "epoch number:70000 train_loss: 1.1968433000220102e-06 val_loss9.883978778022045e-07\n",
      "epoch number:80000 train_loss: 1.1595715250223293e-06 val_loss9.861542457343e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Early stopping is triggered, current best loss: 9.811746930243882e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number:0 train_loss: 1.1511369848449249e-06 val_loss9.898802653191778e-07\n",
      "epoch number:10000 train_loss: 1.1466681826277636e-06 val_loss9.890582565335026e-07\n",
      "epoch number:20000 train_loss: 1.1627405456238193e-06 val_loss9.929246100747242e-07\n",
      "epoch number:30000 train_loss: 1.1384886420273688e-06 val_loss9.905439626828923e-07\n",
      "epoch number:40000 train_loss: 1.1660156360449037e-06 val_loss9.905943308157718e-07\n",
      "epoch number:50000 train_loss: 1.1923727925022831e-06 val_loss9.872863663446795e-07\n",
      "epoch number:60000 train_loss: 1.1758445452869637e-06 val_loss9.880504392699115e-07\n",
      "epoch number:70000 train_loss: 1.2230329957674257e-06 val_loss9.8574262653989e-07\n",
      "epoch number:80000 train_loss: 1.2171357184342924e-06 val_loss9.859711273020525e-07\n",
      "epoch number:90000 train_loss: 1.201191025756998e-06 val_loss9.851937264896424e-07\n",
      "epoch number:100000 train_loss: 1.0836843102879357e-06 val_loss9.831031491896753e-07\n",
      "epoch number:110000 train_loss: 1.1610000001383014e-06 val_loss9.8400267579167e-07\n",
      "epoch number:120000 train_loss: 1.1246966096223332e-06 val_loss9.862361048474764e-07\n",
      "epoch number:130000 train_loss: 1.1582740171434125e-06 val_loss9.801537277370278e-07\n",
      "epoch number:140000 train_loss: 1.0936171292996733e-06 val_loss9.825987233023406e-07\n",
      "epoch number:150000 train_loss: 1.0725278798418003e-06 val_loss9.802172516376125e-07\n",
      "epoch number:160000 train_loss: 1.1622377087405766e-06 val_loss9.812816748108738e-07\n",
      "epoch number:170000 train_loss: 1.1524663250384037e-06 val_loss9.7840740091954e-07\n",
      "epoch number:180000 train_loss: 1.1148192697874038e-06 val_loss9.758006455244148e-07\n",
      "epoch number:190000 train_loss: 1.169188635685714e-06 val_loss9.761014375192129e-07\n",
      "epoch number:200000 train_loss: 1.1440112075433717e-06 val_loss9.748665762561545e-07\n",
      "epoch number:210000 train_loss: 1.0990326018145424e-06 val_loss9.772472377643836e-07\n",
      "epoch number:220000 train_loss: 1.0654248399077915e-06 val_loss9.767021483439306e-07\n",
      "epoch number:230000 train_loss: 1.0556360621194472e-06 val_loss9.756444882888529e-07\n",
      "epoch number:240000 train_loss: 1.1042463938792935e-06 val_loss9.740633767714031e-07\n",
      "epoch number:0 train_loss: 1.0899923381657572e-06 val_loss9.712904898546739e-07\n",
      "epoch number:10000 train_loss: 1.1609122338995803e-06 val_loss9.758757070926895e-07\n",
      "epoch number:20000 train_loss: 1.0668318282114342e-06 val_loss9.745827005598228e-07\n",
      "epoch number:30000 train_loss: 1.0871805216083885e-06 val_loss9.718935159539618e-07\n",
      "epoch number:40000 train_loss: 1.0544817996560596e-06 val_loss9.686762833642049e-07\n",
      "epoch number:50000 train_loss: 1.0592763146632933e-06 val_loss9.725359430678584e-07\n",
      "epoch number:60000 train_loss: 1.1243043900321936e-06 val_loss9.677754785973317e-07\n",
      "epoch number:70000 train_loss: 1.0972078143822728e-06 val_loss9.682265742356303e-07\n",
      "epoch number:80000 train_loss: 1.0301773727405816e-06 val_loss9.677435578409265e-07\n",
      "epoch number:90000 train_loss: 1.0845378710655496e-06 val_loss9.66450561217024e-07\n",
      "epoch number:100000 train_loss: 1.0518081126065226e-06 val_loss9.648047439053344e-07\n",
      "epoch number:110000 train_loss: 1.0997791832778603e-06 val_loss9.655536123348785e-07\n",
      "epoch number:120000 train_loss: 1.0325154562451644e-06 val_loss9.622303821722202e-07\n",
      "epoch number:130000 train_loss: 1.088944941329828e-06 val_loss9.646505402239212e-07\n",
      "epoch number:140000 train_loss: 1.0697793868530425e-06 val_loss9.653076361541114e-07\n",
      "epoch number:150000 train_loss: 1.0672188182070386e-06 val_loss9.59240607004284e-07\n",
      "epoch number:160000 train_loss: 1.0491349939911743e-06 val_loss9.64765296548828e-07\n",
      "epoch number:170000 train_loss: 1.0481651315785712e-06 val_loss9.607462783483318e-07\n",
      "epoch number:180000 train_loss: 9.91421416074445e-07 val_loss9.596443099291206e-07\n",
      "epoch number:190000 train_loss: 1.0388522468929295e-06 val_loss9.623772478086594e-07\n",
      "epoch number:200000 train_loss: 1.0646442660799948e-06 val_loss9.613006293662804e-07\n",
      "epoch number:210000 train_loss: 9.540483461023541e-07 val_loss9.69488895021406e-07\n",
      "epoch number:220000 train_loss: 1.0112582913279766e-06 val_loss9.766058391680607e-07\n",
      "epoch number:230000 train_loss: 1.0553189895290416e-06 val_loss9.730185243203459e-07\n",
      "epoch number:0 train_loss: 1.0373300938226748e-06 val_loss9.74356175353799e-07\n",
      "epoch number:10000 train_loss: 1.0300301482857321e-06 val_loss9.70873535039301e-07\n",
      "epoch number:20000 train_loss: 1.0504795682209078e-06 val_loss9.731307721956636e-07\n",
      "epoch number:30000 train_loss: 9.58584678301122e-07 val_loss9.710123294298885e-07\n",
      "epoch number:40000 train_loss: 1.0727761718953843e-06 val_loss9.730914220150756e-07\n",
      "epoch number:50000 train_loss: 9.67693608799891e-07 val_loss9.695630976864243e-07\n",
      "epoch number:60000 train_loss: 1.0189060049015097e-06 val_loss9.672547041057477e-07\n",
      "epoch number:70000 train_loss: 1.011210315482458e-06 val_loss9.664447201985394e-07\n",
      "epoch number:80000 train_loss: 1.0376052159699611e-06 val_loss9.677988916015943e-07\n",
      "epoch number:90000 train_loss: 9.90164380709757e-07 val_loss9.69602128464449e-07\n",
      "epoch number:100000 train_loss: 9.855552889348473e-07 val_loss9.68419041055177e-07\n",
      "epoch number:110000 train_loss: 9.825722599998699e-07 val_loss9.68986295728234e-07\n",
      "epoch number:120000 train_loss: 9.54765937422053e-07 val_loss9.663571502288168e-07\n",
      "epoch number:130000 train_loss: 9.877967386273667e-07 val_loss9.663675427760513e-07\n",
      "epoch number:140000 train_loss: 9.120192316913744e-07 val_loss9.652682821900564e-07\n",
      "epoch number:150000 train_loss: 9.89224304248637e-07 val_loss9.630791790431734e-07\n",
      "epoch number:160000 train_loss: 9.734643526826403e-07 val_loss9.671126043969637e-07\n",
      "epoch number:170000 train_loss: 1.0173628197662765e-06 val_loss9.653079232052858e-07\n",
      "epoch number:180000 train_loss: 9.62392050496419e-07 val_loss9.665062425848809e-07\n",
      "epoch number:190000 train_loss: 9.642615168559132e-07 val_loss9.654394261514793e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=48, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=15, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=48, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainning process\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "EPOCHS = 100000\n",
    "\n",
    "#prepare data\n",
    "train_len = int(len(dat) * 0.8)\n",
    "val_len = len(dat) - train_len\n",
    "train_dat, val_dat = random_split(dat, [train_len, val_len])\n",
    "writer = SummaryWriter()\n",
    "train_loader = DataLoader(ChronosDataset(train_dat), BATCH_SIZE, True)\n",
    "val_loader = DataLoader(ChronosDataset(val_dat), BATCH_SIZE, True)\n",
    "first_layer = dat.shape[1]\n",
    "layer_1 = max(math.floor(first_layer/4),30) #max(math.floor(n_inputs/4), 30)\n",
    "layer_2 = max(math.floor(layer_1/4),15) #max(math.floor(n_units_1/4), 15)\n",
    "\n",
    "ae = AutoEncoder(first_layer, layer_1, layer_2)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5, patience=125, min_lr = 0.00001, verbose=True)\n",
    "loss_f = nn.MSELoss()\n",
    "model_candidates = {}\n",
    "\n",
    "for i in range(9):\n",
    "    tolerence_percentage = (i + 1) / 10.0\n",
    "    tolerence = EPOCHS * tolerence_percentage\n",
    "    best_ae, best_loss = _fit(ae, train_loader, val_loader, EPOCHS, BATCH_SIZE, tolerence, optimizer, scheduler, loss_f, writer)\n",
    "    filename = f\"temperary_model/temp_model_{i}\"\n",
    "    key = best_loss.item()\n",
    "    if key not in model_candidates:\n",
    "        model_candidates[key] = filename\n",
    "        try:\n",
    "            torch.save(best_ae,filename)\n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "\n",
    "keys = np.array(list(model_candidates.keys()))\n",
    "min_loss = keys.min()\n",
    "ae = torch.load(model_candidates[min_loss])\n",
    "ae.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2940f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.52170632e-06 1.13303536e-06 1.06088157e-06 9.94807295e-07\n",
      " 9.79871809e-07 9.81174693e-07 9.67290743e-07 9.54962694e-07\n",
      " 9.60579879e-07]\n",
      "9.549626944165708e-07\n"
     ]
    }
   ],
   "source": [
    "print(keys)\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ec6cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ingest and preprocess data for scoring\n",
    "\n",
    "A lot of this is similar to preparing the training data, the main differences are that\n",
    "\n",
    "1. We do not remove outliers.\n",
    "2. We need to leftpad the data with the median_profiles so that we can generate scores for the first time point in the actual data.\n",
    "\n",
    "Note that some of the following cells overwrite existing variable names, so rerunning preceding cells will yield inconsistent results.\n",
    "\n",
    "To score other files, simply change the value of `scorefname` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftpad(df, ycols, minfreq, period, profile):\n",
    "    \"\"\"leftpads dataframe by period-1 points with median values for each column\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    firstindex = df['time_index'][0]\n",
    "    leftpad_index = [i%period for i in range(firstindex - (period - 1), firstindex)]\n",
    "    dat = {y:[profile.loc[i][y] for i in leftpad_index] for y in ycols}\n",
    "    dat['time_index'] = leftpad_index\n",
    "    leftdf = pd.DataFrame(\n",
    "        data = dat,\n",
    "        index = pd.date_range(end=df.index[0], periods=period, freq=minfreq, closed='left'))\n",
    "    df = leftdf.append(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorefname = fname # substitute to score different data\n",
    "df = ingest(scorefname, datecol, ycols)\n",
    "df, _, _ = ae_preproc(df, ycols)\n",
    "original_data = df.copy() # saved because we might need it later\n",
    "\n",
    "aligned, _, _ = align_data(original_data, colfreqs, minfreq, anchors, zeropoint)\n",
    "aligned['time_index'] = [((t-zeropoint)//minfreq)%period for t in aligned.index]\n",
    "df = leftpad(aligned, ycols, minfreq, period, median_profiles)\n",
    "df = fill_gaps(df, colfreqs, minfreq, period, median_profiles, zeropoint)\n",
    "test_seq = aligned.copy()\n",
    "nona = test_seq.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb9280",
   "metadata": {},
   "source": [
    "## Load model for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = construct_ae_model(modelmeta['input_size'])\n",
    "model.load_weights(modfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b13c35",
   "metadata": {},
   "source": [
    "## Actual scoring\n",
    "\n",
    "Here we score the data in batches. Each batch is all the windows from one phase in the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in time_index:\n",
    "    times = nona[nona['time_index']==t].index\n",
    "    signals = generate_model_input(df, ycols, times, period, modnorms, offset=t, median_values=median_profiles)\n",
    "    preds = model.predict(signals.reshape([signals.shape[0], -1]))\n",
    "    errors = np.sum(np.square(signals-preds), axis=1)\n",
    "    test_seq.loc[times, 'score'] = errors\n",
    "    #logging.info(f\"finished scoring window {t}\")\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "test_seq[['score']].plot(figsize=[16,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in time_index:\n",
    "    times = nona[nona['time_index']==t].index\n",
    "    signals = generate_model_input(df, ycols, times, period, modnorms, offset=t, median_values=median_profiles)\n",
    "    _signals = signals.reshape([signals.shape[0], -1])\n",
    "\n",
    "    _signals = torch.from_numpy(signals)\n",
    "    signals = Variable(_signals.float())\n",
    "    code,preds = ae(signals)\n",
    "    errors = np.sum(np.square((signals-preds).detach().numpy()), axis=1)\n",
    "    test_seq.loc[times, 'score'] = errors\n",
    "    #logging.info(f\"finished scoring window {t}\")\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "test_seq[['score']].plot(figsize=[16,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f31889",
   "metadata": {},
   "source": [
    "## Zoomed in views\n",
    "\n",
    "This has nothing to do with modelling, really. In actual deployment, we allow the users to pick a point _t_ in time and look at the actual data and the values reconstructed by the autoencoder so that they can decide whether or not it was an anomaly. There are two parts to this.\n",
    "\n",
    "1. Run the model on the window of 1 period ending at _t_ to generate the outputs. This is mostly the same process as in scoring.\n",
    "2. Reconstruct the autoencoder output (basically a vector) into the columns. This is the reverse process of `generate_model_input()`.\n",
    "\n",
    "To generate views for different csvs and times, just change `zoomfname` and `ts` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormdf(normed, modnorms):\n",
    "    df = normed.copy()\n",
    "    for y, ( ymin, ymax ) in modnorms.items():\n",
    "        mmdiff = ymax - ymin\n",
    "        df[y] = df[y]*mmdiff + ymin\n",
    "    return df\n",
    "\n",
    "def unpack_pred_row(df, ycols, t, period, preds, offset):\n",
    "    \"\"\"\n",
    "unpacks the predicted values numpy array into their respective columns\n",
    "\n",
    "these values are placed in their proper rows in df as \"{y}_pred\"\n",
    "    \"\"\"\n",
    "    idx = df.index.get_loc(t)+1\n",
    "    rowdf = df.iloc[idx-period:idx].copy()\n",
    "    original = rowdf.copy()\n",
    "    startidx = 0\n",
    "    for y in ycols:\n",
    "        ynona = rowdf[[y]].dropna()\n",
    "        ylen = len(ynona)\n",
    "        ypreds = preds[startidx:startidx+ylen]\n",
    "        rowdf.loc[ynona.index, f'{y}_preds'] = ypreds\n",
    "        startidx = startidx + ylen\n",
    "    startpos = 1+offset\n",
    "    rowdf = rowdf.iloc[startpos:].append(rowdf.iloc[:startpos])\n",
    "    rowdf = rowdf.reset_index(drop=True).set_index(original.index)\n",
    "    for y in ycols:\n",
    "        rowdf[y] = original[y]\n",
    "    return rowdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomfname = fname\n",
    "ts = \"2019-10-28T15:30\"\n",
    "\n",
    "\n",
    "df = ingest(zoomfname, datecol, ycols)\n",
    "df, _, _ = ae_preproc(df, ycols)\n",
    "original_data = df.copy() # saved because we might need it later\n",
    "\n",
    "\n",
    "aligned, _, _ = align_data(original_data, colfreqs, minfreq, anchors, zeropoint)\n",
    "aligned['time_index'] = [((t-zeropoint)//minfreq)%period for t in aligned.index]\n",
    "df = leftpad(aligned, ycols, minfreq, period, median_profiles)\n",
    "df = fill_gaps(df, colfreqs, minfreq, period, median_profiles, zeropoint)\n",
    "\n",
    "\n",
    "model = construct_ae_model(modelmeta['input_size'])\n",
    "model.load_weights(modfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e580e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.Timestamp(ts)\n",
    "modt = time_index[-1]\n",
    "t = df.loc[idx, 'time_index']\n",
    "\n",
    "prednorms = {f\"{y}_preds\": modnorms[y] for y in modnorms}\n",
    "vals = generate_model_input(df, ycols, [idx], period, modnorms, offset=t, median_values=median_profiles)\n",
    "preds = model.predict(vals)\n",
    "rowdf = unpack_pred_row(df, ycols, idx, period, preds[0], t)\n",
    "# denormalize data\n",
    "rowdf = denormdf(rowdf, prednorms)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed view for new PyTorch model\n",
    "idx = pd.Timestamp(ts)\n",
    "modt = time_index[-1]\n",
    "t = df.loc[idx, 'time_index']\n",
    "\n",
    "prednorms = {f\"{y}_preds\": modnorms[y] for y in modnorms}\n",
    "vals = generate_model_input(df, ycols, [idx], period, modnorms, offset=t, median_values=median_profiles)\n",
    "_vals = torch.from_numpy(vals)\n",
    "_vals = Variable(_vals.float())\n",
    "code,preds_pt = ae(_vals)\n",
    "rowdf_pt = unpack_pred_row(df, ycols, idx, period, preds_pt.detach().numpy()[0], t)\n",
    "# denormalize data\n",
    "rowdf_pt = denormdf(rowdf_pt, prednorms)\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in ycols:\n",
    "    rowdf[f'{y}_new_preds'] = rowdf_pt[f\"{y}_preds\"]\n",
    "    \n",
    "for y in ycols:\n",
    "    rowdf[[y, f\"{y}_preds\",f'{y}_new_preds']].plot(figsize=[16,8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

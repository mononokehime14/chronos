{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "virgin-stability",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook duplicates the data preprocessing and modelling that Chronos+ does, with all the database and job queue code stripped out. As far as possible, the duplication is verbatim to facilitate the transfer of any changes made in this notebook back into Chronos+. All current data rejection points are also documented.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crucial-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.tsa.stattools as sts\n",
    "from scipy.signal import argrelextrema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-recall",
   "metadata": {},
   "source": [
    "## Define CSV filename, datetime and value columns\n",
    "\n",
    "To test the modelling and scoring process on your own data, just change the following variables.\n",
    "\n",
    "1. `datecol` --> the name of the column in the CSV file we wish to use as the time series index.\n",
    "2. `ycols` --> the list of names of the value columns we wish to train and score on.\n",
    "3. `fname` --> name of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atomic-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "datecol = \"Time\"\n",
    "ycols = [\"SA1900282\"] # there can be more than one\n",
    "fname = \"data/giordano-oct-nov.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-potato",
   "metadata": {},
   "source": [
    "## Ingesting the data\n",
    "\n",
    "1. `datecol` must exist in the CSV or we raise an exception\n",
    "2. `ycols` must contain the name of at least one column or we raise an exception\n",
    "3. every column in `ycols` must exist in the CSV or we raise an exception\n",
    "4. if we cannot parse a value in the datetime column as a datetime, we discard the whole row\n",
    "5. every value in the columns denoted by `ycols` is parsed as a float or set to np.nan\n",
    "6. we set the `datecol` column as the index of the dataframe\n",
    "7. we sort the dataframe by the index\n",
    "7. we drop any columns not listed in `ycols` and return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vital-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingColumnError(Exception):\n",
    "    def __init__(self, col, *args, **kwargs):\n",
    "        self.column = col\n",
    "        Exception.__init__(self, *args, **kwargs)\n",
    "        \n",
    "def ingest(csv, dscol, ycols):\n",
    "    \"\"\" ingests a timeseries csv. Does not throw an error if datetime format is not followed (will always try to interpret) or if value column contains non-int or non-floats.\n",
    "\n",
    "    1) renames the datetime column to 'ds'\n",
    "\n",
    "    2) indexes the 'ds' column\n",
    "\n",
    "    3) sorts by ds column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    csv : str \n",
    "         path to csv with header and format `datetime,value`.\n",
    "         datetime: {%Y-%m-%d (e.g. 2017-12-01), \n",
    "                    %Y-%m-%d (e.g. 2017-12-1), \n",
    "                    %Y-%b-%d (e.g. 2017-Dec-01), \n",
    "                    %d-%b-%Y (e.g. 1-Dec-2017), \n",
    "                    %Y-%m-%d %H:%M:%S (e.g. 2017-12-01 00:00:00)}\n",
    "                 * note all represent 1st December 2017\n",
    "         value: {float, int}\n",
    "\n",
    "    dscol : str\n",
    "                    the header name for the datetime column in the csv\n",
    "    ycols: list of str\n",
    "                    the names of the value columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    if dscol not in df:\n",
    "        raise MissingColumnError(dscol)\n",
    "\n",
    "    if not ycols:\n",
    "        raise ValueError(f\"ycols is empty: {ycols}\")\n",
    "\n",
    "    for ycol in ycols:\n",
    "        if ycol not in df:\n",
    "            raise MissingColumnError(ycol)\n",
    "\n",
    "    # if the date column is not parseable as a date, then we should discard the row\n",
    "    df[dscol] = pd.to_datetime(df[dscol], errors=\"coerce\")\n",
    "    df = df[~df[dscol].isnull()]\n",
    "\n",
    "    # parse value column as float. no exceptions\n",
    "    for ycol in ycols:\n",
    "        df[ycol] = df[ycol].apply(__float_or_nan)\n",
    "\n",
    "    df = df.set_index(dscol).sort_index()[ycols]\n",
    "    return df.sort_index()\n",
    "\n",
    "def __float_or_nan(o):\n",
    "    try:\n",
    "        return np.float(o)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest(fname, datecol, ycols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-paintball",
   "metadata": {},
   "source": [
    "## Autoencoder preprocessing\n",
    "\n",
    "This is code that needs to be run regardless of whether we are preprocessing data for training or scoring. That is why it has been refactored into its own function.\n",
    "\n",
    "1. remove duplicate rows (identical datetime and values)\n",
    "2. if we find rows with identical datetime but different values, we raise an exception because it is impossible for us to decide which row contains the correct value.\n",
    "  1. the other option is to delete all such rows, but for now we have elected to reject the entire CSV\n",
    "3. we try to detect the sampling frequency of each value column\n",
    "  1. we take the mode of the time differences between successive values for each column\n",
    "  2. if there is no mode for any of the columns we reject the whole CSV\n",
    "4. we return the resulting dataframe, along with the sampling frequencies for each column, as well as the minimum sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "resident-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_preproc(df, ycols):\n",
    "    \"\"\"takes df, ycols and returns preprocessed df, freq\"\"\"\n",
    "    df = remove_duplicate_rows(df)\n",
    "\n",
    "    dupe_ds = list(df[df.index.duplicated()].index.drop_duplicates())\n",
    "    # we also reject if we have duplicate datetimes that are not duplicate rows\n",
    "    if len(dupe_ds)!=0:\n",
    "        raise Exception(\"rows with duplicate datetimes detected\")\n",
    "\n",
    "    colfreqs = {}\n",
    "    for y in ycols:\n",
    "        currdf = df[[y]].dropna()\n",
    "        freqs = detect_ds_frequency(currdf)\n",
    "        if len(freqs)!=1:\n",
    "            # we reject if we have no modal sampling frequency\n",
    "            raise Exception(\"more than one sampling frequency was detected\")\n",
    "\n",
    "        colfreqs[y] = freqs.index[0]\n",
    "\n",
    "    minfreq = min(colfreqs.values())\n",
    "\n",
    "    return df, colfreqs, minfreq\n",
    "\n",
    "def remove_duplicate_rows(df):\n",
    "    \"\"\"returns df with duplicates removed \n",
    "    \"\"\"    \n",
    "    idx_name = df.index.name\n",
    "    return df.reset_index().drop_duplicates().set_index(idx_name)\n",
    "\n",
    "def detect_ds_frequency(df):\n",
    "    \"\"\"detects the frequency of datetime in df.\n",
    "    \n",
    "    1) subtract all the datetime in ds by its next datetime to retrieve the deltas\n",
    "    \n",
    "    2) count the number of occurrences for each delta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame \n",
    "         The dataframe to detect the datetime frequency, needs to have a sorted datetime index\n",
    "    Returns\n",
    "    -------\n",
    "    freq : pd.Series of size 1 whose index is the modal timedelta and value is the number of times it appeared if there is one mode\n",
    "    and it appeared > 50% of the time\n",
    "           else pd.Series of max size 10 sorted by ascending order of modal timedeltas\n",
    "    \"\"\"\n",
    "    deltas = pd.Series(df.index[1:] - df.index[:-1])\n",
    "    threshold = len(deltas)/2\n",
    "    delta_counts = deltas.value_counts()\n",
    "    modal_delta = delta_counts.idxmax()\n",
    "    mdcount = delta_counts[modal_delta]\n",
    "\n",
    "    if mdcount > threshold:\n",
    "        return delta_counts.head(1)\n",
    "\n",
    "    dc = [item for item in delta_counts.iteritems()]\n",
    "    # sort by ascending order of timedelta\n",
    "    dc.sort(key=lambda e:e[0])\n",
    "    # sort by descending order of occurrences\n",
    "    dc.sort(key=lambda e:e[1], reverse=True)\n",
    "    dc = dc[:50]\n",
    "\n",
    "    return pd.Series(data=[e[1] for e in dc], index=[e[0] for e in dc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "packed-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, colfreqs, minfreq = ae_preproc(df, ycols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-teach",
   "metadata": {},
   "source": [
    "## Check for sparse columns\n",
    "\n",
    "Given a duration _d_ and a time series with a sampling frequency _f_, the expected number of values it should have over _d_ is \n",
    "\n",
    "_e_ = _d_/_f_\n",
    "\n",
    "We define a sparse column as a column within the dataframe where given its sampling frequency and the duration of the whole dataframe, it has less than 75% the number of expected values. We raise an exception if any column is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "several-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseColumnError(Exception):\n",
    "    pass\n",
    "\n",
    "def check_sparse_cols(df, colfreqs):\n",
    "    totalduration = df.index[-1] - df.index[0]\n",
    "    for y, freq in colfreqs.items():\n",
    "        numpoints = len(df[[y]].dropna())\n",
    "        # every column must cover 75% of the duration\n",
    "        expected = totalduration // freq * 0.75\n",
    "        if numpoints < expected:\n",
    "            raise SparseColumnError(f\"column {y} has frequency {freq} and {numpoints} points, but duration of dataset is {totalduration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nonprofit-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sparse_cols(df, colfreqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-heart",
   "metadata": {},
   "source": [
    "## Get period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "further-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPeriodError(Exception):\n",
    "    pass\n",
    "\n",
    "def get_period(df, colfreqs, minfreq):\n",
    "    # check that all sampling frequencies are multiples of the smallest sampling frequency\n",
    "    notime = pd.Timedelta(0)\n",
    "    for f in colfreqs.values():\n",
    "        if f % minfreq != notime:\n",
    "            raise ValueError(\"detected sampling frequency that is not a multiple of the minimum sampling frequency\")\n",
    "\n",
    "    # there is no point calculating autocorrelation for lags greater than n/2\n",
    "    nlags = len(df)//2\n",
    "    acfs = np.zeros((nlags+1,))\n",
    "    for y in colfreqs:\n",
    "        yvals = df[y].dropna()\n",
    "        yacfs = sts.acf(yvals, nlags=len(yvals)//2)\n",
    "        step_size = colfreqs[y]//minfreq\n",
    "        yacfs_idx = range(0, len(acfs), step_size)\n",
    "        # we simplify by cropping because if len(df) % len(yvals) != step_size we get tedious off-by-one errors\n",
    "        yacfs = yacfs[:len(yacfs_idx)]\n",
    "        # if y has missing values, yacfs can be shorter than yacfs_idx -_-\n",
    "        yacfs_idx = yacfs_idx[:len(yacfs)]\n",
    "        acfs[yacfs_idx] += yacfs\n",
    "\n",
    "    max_corr_points = argrelextrema(acfs, np.greater, order=max(len(df)//1000, 2))\n",
    "    max_corr_points = max_corr_points[0]\n",
    "    max_corr_points = max_corr_points[acfs[max_corr_points]>0.2]\n",
    "    max_corr_points = np.insert(max_corr_points, 0, 0, axis=0)\n",
    "    max_cor_diff = []\n",
    "    for point_1 in max_corr_points:\n",
    "        for point_2 in max_corr_points:\n",
    "            if point_1==point_2: continue\n",
    "            max_cor_diff.append(abs(point_1-point_2))\n",
    "    max_cor_diff = np.array(max_cor_diff)\n",
    "    unique_vals, counts = np.unique(max_cor_diff, return_counts=True)\n",
    "    adjust_counts = []\n",
    "    for idx in range(len(unique_vals)):\n",
    "        adjust_counts.append(np.sum(counts[np.where(unique_vals%unique_vals[idx]==0)]))\n",
    "    if(np.max(adjust_counts)>5):\n",
    "        return unique_vals[np.argmax(adjust_counts)]\n",
    "    raise NoPeriodError(\"no period detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "illegal-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/statsmodels/tsa/stattools.py:541: FutureWarning: fft=True will become the default in a future version of statsmodels. To suppress this warning, explicitly set fft=False.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "period = get_period(df, colfreqs, minfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-parcel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
